{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Data preparation pipeline covering:\n",
    "1) Load from CSV / Excel / SQL\n",
    "2) Understand (head, info, describe, missing values)\n",
    "3) Clean (missing values, duplicates, data types)\n",
    "4) Transform (standardization + categorical encoding)\n",
    "5) Feature engineering + simple feature selection\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import argparse\n",
    "import csv\n",
    "import json\n",
    "import math\n",
    "import re\n",
    "import sqlite3\n",
    "import zipfile\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "from typing import Dict, Iterable, Iterator, List, Optional, Sequence, Tuple\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "\n",
    "SHEET_NS = \"http://schemas.openxmlformats.org/spreadsheetml/2006/main\"\n",
    "REL_NS = \"http://schemas.openxmlformats.org/package/2006/relationships\"\n",
    "WB_NS = \"http://schemas.openxmlformats.org/officeDocument/2006/relationships\"\n",
    "\n",
    "NS_SHEET = {\"a\": SHEET_NS}\n",
    "NS_REL = {\"r\": REL_NS}\n",
    "NS_WB = {\"a\": SHEET_NS, \"r\": WB_NS}\n",
    "\n",
    "\n",
    "def qi(identifier: str) -> str:\n",
    "    return '\"' + identifier.replace('\"', '\"\"') + '\"'\n",
    "\n",
    "\n",
    "def normalize_header(header: str) -> str:\n",
    "    text = re.sub(r\"\\s+\", \"_\", str(header).strip().lower())\n",
    "    text = re.sub(r\"[^0-9a-zA-Z_]+\", \"_\", text).strip(\"_\")\n",
    "    if not text:\n",
    "        text = \"column\"\n",
    "    if text[0].isdigit():\n",
    "        text = f\"c_{text}\"\n",
    "    return text\n",
    "\n",
    "\n",
    "def make_unique_headers(headers: Sequence[str]) -> List[str]:\n",
    "    out: List[str] = []\n",
    "    used: Dict[str, int] = {}\n",
    "    for header in headers:\n",
    "        base = normalize_header(header)\n",
    "        if base not in used:\n",
    "            used[base] = 1\n",
    "            out.append(base)\n",
    "            continue\n",
    "        used[base] += 1\n",
    "        out.append(f\"{base}_{used[base]}\")\n",
    "    return out\n",
    "\n",
    "\n",
    "def col_to_index(cell_ref: str) -> int:\n",
    "    letters = \"\".join(ch for ch in cell_ref if ch.isalpha()).upper()\n",
    "    val = 0\n",
    "    for ch in letters:\n",
    "        val = val * 26 + (ord(ch) - ord(\"A\") + 1)\n",
    "    return max(val - 1, 0)\n",
    "\n",
    "\n",
    "def get_shared_strings(zf: zipfile.ZipFile) -> List[str]:\n",
    "    if \"xl/sharedStrings.xml\" not in zf.namelist():\n",
    "        return []\n",
    "    root = ET.fromstring(zf.read(\"xl/sharedStrings.xml\"))\n",
    "    values = []\n",
    "    for si in root.findall(\"a:si\", NS_SHEET):\n",
    "        txt = \"\".join(t.text or \"\" for t in si.findall(\".//a:t\", NS_SHEET))\n",
    "        values.append(txt)\n",
    "    return values\n",
    "\n",
    "\n",
    "def get_sheet_path(zf: zipfile.ZipFile, sheet_name: Optional[str]) -> str:\n",
    "    workbook = ET.fromstring(zf.read(\"xl/workbook.xml\"))\n",
    "    sheets = workbook.findall(\"a:sheets/a:sheet\", NS_WB)\n",
    "    if not sheets:\n",
    "        raise ValueError(\"No sheets found in workbook.\")\n",
    "\n",
    "    selected_rel_id: Optional[str] = None\n",
    "    if sheet_name:\n",
    "        for sheet in sheets:\n",
    "            if sheet.attrib.get(\"name\") == sheet_name:\n",
    "                selected_rel_id = sheet.attrib.get(\n",
    "                    \"{http://schemas.openxmlformats.org/officeDocument/2006/relationships}id\"\n",
    "                )\n",
    "                break\n",
    "        if selected_rel_id is None:\n",
    "            available = [s.attrib.get(\"name\", \"\") for s in sheets]\n",
    "            raise ValueError(f\"Sheet '{sheet_name}' not found. Available: {available}\")\n",
    "    else:\n",
    "        selected_rel_id = sheets[0].attrib.get(\n",
    "            \"{http://schemas.openxmlformats.org/officeDocument/2006/relationships}id\"\n",
    "        )\n",
    "\n",
    "    rels = ET.fromstring(zf.read(\"xl/_rels/workbook.xml.rels\"))\n",
    "    target = None\n",
    "    for rel in rels.findall(\"r:Relationship\", NS_REL):\n",
    "        if rel.attrib.get(\"Id\") == selected_rel_id:\n",
    "            target = rel.attrib.get(\"Target\")\n",
    "            break\n",
    "    if not target:\n",
    "        raise ValueError(\"Could not resolve worksheet path in workbook relations.\")\n",
    "    return target if target.startswith(\"xl/\") else f\"xl/{target}\"\n",
    "\n",
    "\n",
    "def read_cell_value(cell: ET.Element, shared: Sequence[str]) -> str:\n",
    "    ctype = cell.attrib.get(\"t\")\n",
    "    if ctype == \"inlineStr\":\n",
    "        t = cell.find(\"a:is/a:t\", NS_SHEET)\n",
    "        return (t.text or \"\") if t is not None else \"\"\n",
    "\n",
    "    v = cell.find(\"a:v\", NS_SHEET)\n",
    "    if v is None:\n",
    "        return \"\"\n",
    "    text = v.text or \"\"\n",
    "    if ctype == \"s\" and text.isdigit():\n",
    "        idx = int(text)\n",
    "        if 0 <= idx < len(shared):\n",
    "            return shared[idx]\n",
    "    return text\n",
    "\n",
    "\n",
    "def iter_xlsx_rows(path: Path, sheet_name: Optional[str]) -> Iterator[Dict[str, str]]:\n",
    "    with zipfile.ZipFile(path) as zf:\n",
    "        shared = get_shared_strings(zf)\n",
    "        ws_path = get_sheet_path(zf, sheet_name)\n",
    "        headers: List[str] = []\n",
    "\n",
    "        row_tag = f\"{{{SHEET_NS}}}row\"\n",
    "        cell_tag = f\"{{{SHEET_NS}}}c\"\n",
    "\n",
    "        with zf.open(ws_path) as ws:\n",
    "            for _, elem in ET.iterparse(ws, events=(\"end\",)):\n",
    "                if elem.tag != row_tag:\n",
    "                    continue\n",
    "\n",
    "                cells: Dict[int, str] = {}\n",
    "                for cell in elem.findall(cell_tag):\n",
    "                    ref = cell.attrib.get(\"r\", \"\")\n",
    "                    idx = col_to_index(ref) if ref else len(cells)\n",
    "                    cells[idx] = read_cell_value(cell, shared)\n",
    "\n",
    "                if cells:\n",
    "                    max_idx = max(cells.keys())\n",
    "                    ordered = [cells.get(i, \"\") for i in range(max_idx + 1)]\n",
    "                    if not headers:\n",
    "                        headers = make_unique_headers(ordered)\n",
    "                    else:\n",
    "                        if len(ordered) < len(headers):\n",
    "                            ordered.extend([\"\"] * (len(headers) - len(ordered)))\n",
    "                        yield {headers[i]: ordered[i] if i < len(ordered) else \"\" for i in range(len(headers))}\n",
    "\n",
    "                elem.clear()\n",
    "\n",
    "\n",
    "def iter_csv_rows(path: Path) -> Iterator[Dict[str, str]]:\n",
    "    with path.open(\"r\", encoding=\"utf-8-sig\", newline=\"\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        if reader.fieldnames is None:\n",
    "            raise ValueError(\"CSV has no header row.\")\n",
    "        norm_headers = make_unique_headers(reader.fieldnames)\n",
    "        original = list(reader.fieldnames)\n",
    "        for row in reader:\n",
    "            out = {}\n",
    "            for i, src in enumerate(original):\n",
    "                out[norm_headers[i]] = (row.get(src) or \"\").strip()\n",
    "            yield out\n",
    "\n",
    "\n",
    "def iter_sql_rows(path: Path, query: str) -> Iterator[Dict[str, str]]:\n",
    "    conn = sqlite3.connect(path)\n",
    "    conn.row_factory = sqlite3.Row\n",
    "    try:\n",
    "        cur = conn.execute(query)\n",
    "        cols = [normalize_header(d[0]) for d in cur.description]\n",
    "        cols = make_unique_headers(cols)\n",
    "        for rec in cur:\n",
    "            out = {}\n",
    "            for i, col in enumerate(cols):\n",
    "                val = rec[i]\n",
    "                out[col] = \"\" if val is None else str(val).strip()\n",
    "            yield out\n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "\n",
    "def guess_source_type(path: Path, source_type: str) -> str:\n",
    "    if source_type != \"auto\":\n",
    "        return source_type\n",
    "    suffix = path.suffix.lower()\n",
    "    if suffix == \".csv\":\n",
    "        return \"csv\"\n",
    "    if suffix in {\".xlsx\", \".xlsm\"}:\n",
    "        return \"excel\"\n",
    "    if suffix in {\".db\", \".sqlite\", \".sqlite3\"}:\n",
    "        return \"sql\"\n",
    "    raise ValueError(f\"Cannot infer source type for file: {path}\")\n",
    "\n",
    "\n",
    "def parse_float(value: object) -> Optional[float]:\n",
    "    if value is None:\n",
    "        return None\n",
    "    text = str(value).strip()\n",
    "    if text == \"\":\n",
    "        return None\n",
    "    try:\n",
    "        return float(text)\n",
    "    except ValueError:\n",
    "        cleaned = re.sub(r\"[^0-9.\\-]\", \"\", text)\n",
    "        if cleaned in {\"\", \"-\", \".\", \"-.\"}:\n",
    "            return None\n",
    "        try:\n",
    "            return float(cleaned)\n",
    "        except ValueError:\n",
    "            return None\n",
    "\n",
    "\n",
    "def excel_serial_to_datetime(serial: float) -> datetime:\n",
    "    base = datetime(1899, 12, 30)\n",
    "    return base + timedelta(days=float(serial))\n",
    "\n",
    "\n",
    "def parse_datetime(value: object) -> Optional[str]:\n",
    "    if value is None:\n",
    "        return None\n",
    "    text = str(value).strip()\n",
    "    if text == \"\":\n",
    "        return None\n",
    "\n",
    "    maybe_float = parse_float(text)\n",
    "    if maybe_float is not None and 20_000 <= maybe_float <= 80_000:\n",
    "        try:\n",
    "            dt = excel_serial_to_datetime(maybe_float)\n",
    "            return dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    patterns = [\n",
    "        \"%Y-%m-%d %H:%M:%S\",\n",
    "        \"%Y-%m-%d %H:%M\",\n",
    "        \"%Y-%m-%d\",\n",
    "        \"%d-%m-%Y %H:%M\",\n",
    "        \"%d-%m-%Y\",\n",
    "        \"%m/%d/%Y %H:%M\",\n",
    "        \"%m/%d/%Y\",\n",
    "        \"%d/%m/%Y %H:%M\",\n",
    "        \"%d/%m/%Y\",\n",
    "    ]\n",
    "    for pattern in patterns:\n",
    "        try:\n",
    "            dt = datetime.strptime(text, pattern)\n",
    "            return dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        except ValueError:\n",
    "            continue\n",
    "    return None\n",
    "\n",
    "\n",
    "def safe_real(value: object) -> Optional[float]:\n",
    "    return parse_float(value)\n",
    "\n",
    "\n",
    "def is_id_like(name: str) -> bool:\n",
    "    low = name.lower()\n",
    "    return any(token in low for token in (\"_id\", \"id\", \"code\", \"no\", \"number\"))\n",
    "\n",
    "\n",
    "def slugify(text: str, max_len: int = 28) -> str:\n",
    "    s = re.sub(r\"[^0-9a-zA-Z]+\", \"_\", str(text).strip().lower()).strip(\"_\")\n",
    "    if not s:\n",
    "        s = \"value\"\n",
    "    return s[:max_len]\n",
    "\n",
    "\n",
    "def infer_types(col_stats: Dict[str, Dict[str, object]]) -> Dict[str, str]:\n",
    "    inferred: Dict[str, str] = {}\n",
    "    for col, st in col_stats.items():\n",
    "        non_missing = int(st[\"non_missing\"])\n",
    "        numeric_hits = int(st[\"numeric_hits\"])\n",
    "        datetime_hits = int(st[\"datetime_hits\"])\n",
    "\n",
    "        name = col.lower()\n",
    "        numeric_ratio = numeric_hits / max(non_missing, 1)\n",
    "        datetime_ratio = datetime_hits / max(non_missing, 1)\n",
    "\n",
    "        if \"date\" in name or \"time\" in name:\n",
    "            inferred[col] = \"datetime\"\n",
    "        elif is_id_like(name):\n",
    "            inferred[col] = \"categorical\"\n",
    "        elif numeric_ratio >= 0.9:\n",
    "            inferred[col] = \"numeric\"\n",
    "        elif datetime_ratio >= 0.9:\n",
    "            inferred[col] = \"datetime\"\n",
    "        else:\n",
    "            inferred[col] = \"categorical\"\n",
    "    return inferred\n",
    "\n",
    "\n",
    "def describe_numeric_column(conn: sqlite3.Connection, table: str, col: str) -> Optional[Dict[str, float]]:\n",
    "    qcol = qi(col)\n",
    "    n = conn.execute(f\"SELECT COUNT({qcol}) FROM {table} WHERE {qcol} IS NOT NULL\").fetchone()[0]\n",
    "    if not n:\n",
    "        return None\n",
    "\n",
    "    avg_, avg_sq, min_, max_ = conn.execute(\n",
    "        f\"SELECT AVG({qcol}), AVG({qcol} * {qcol}), MIN({qcol}), MAX({qcol}) FROM {table} WHERE {qcol} IS NOT NULL\"\n",
    "    ).fetchone()\n",
    "    variance = max((avg_sq or 0.0) - (avg_ or 0.0) ** 2, 0.0)\n",
    "    std_ = math.sqrt(variance)\n",
    "\n",
    "    def percentile(p: float) -> float:\n",
    "        idx = int((n - 1) * p)\n",
    "        row = conn.execute(\n",
    "            f\"SELECT {qcol} FROM {table} WHERE {qcol} IS NOT NULL ORDER BY {qcol} LIMIT 1 OFFSET ?\",\n",
    "            (idx,),\n",
    "        ).fetchone()\n",
    "        return float(row[0]) if row else float(\"nan\")\n",
    "\n",
    "    return {\n",
    "        \"count\": float(n),\n",
    "        \"mean\": float(avg_ or 0.0),\n",
    "        \"std\": float(std_),\n",
    "        \"min\": float(min_ or 0.0),\n",
    "        \"25%\": percentile(0.25),\n",
    "        \"50%\": percentile(0.50),\n",
    "        \"75%\": percentile(0.75),\n",
    "        \"max\": float(max_ or 0.0),\n",
    "    }\n",
    "\n",
    "\n",
    "def export_table_csv(conn: sqlite3.Connection, table: str, out_path: Path, limit: Optional[int] = None) -> None:\n",
    "    sql = f\"SELECT * FROM {table}\"\n",
    "    if limit is not None:\n",
    "        sql += f\" LIMIT {int(limit)}\"\n",
    "    cur = conn.execute(sql)\n",
    "    headers = [d[0] for d in cur.description]\n",
    "\n",
    "    with out_path.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(headers)\n",
    "        for row in cur:\n",
    "            writer.writerow(row)\n",
    "\n",
    "\n",
    "def build_row_iterator(args: argparse.Namespace) -> Iterator[Dict[str, str]]:\n",
    "    src = Path(args.input)\n",
    "    source_type = guess_source_type(src, args.source_type)\n",
    "    if source_type == \"csv\":\n",
    "        return iter_csv_rows(src)\n",
    "    if source_type == \"excel\":\n",
    "        return iter_xlsx_rows(src, args.sheet)\n",
    "    if source_type == \"sql\":\n",
    "        if args.sql_query:\n",
    "            query = args.sql_query\n",
    "        elif args.sql_table:\n",
    "            query = f\"SELECT * FROM {args.sql_table}\"\n",
    "        else:\n",
    "            raise ValueError(\"SQL source requires --sql-query or --sql-table.\")\n",
    "        return iter_sql_rows(src, query)\n",
    "    raise ValueError(f\"Unsupported source type: {source_type}\")\n",
    "\n",
    "\n",
    "def run_pipeline(args: argparse.Namespace) -> Dict[str, object]:\n",
    "    source_path = Path(args.input).expanduser()\n",
    "    output_dir = Path(args.output_dir).expanduser()\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    db_path = output_dir / \"prep_work.sqlite\"\n",
    "    if db_path.exists():\n",
    "        db_path.unlink()\n",
    "\n",
    "    rows = build_row_iterator(args)\n",
    "    first = next(rows, None)\n",
    "    if first is None:\n",
    "        raise ValueError(\"No rows found in source.\")\n",
    "\n",
    "    headers = list(first.keys())\n",
    "    columns = make_unique_headers(headers)\n",
    "\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    conn.create_function(\"safe_real\", 1, safe_real)\n",
    "    conn.create_function(\"parse_datetime\", 1, parse_datetime)\n",
    "\n",
    "    try:\n",
    "        conn.execute(\n",
    "            \"CREATE TABLE raw (\" + \", \".join(f\"{qi(col)} TEXT\" for col in columns) + \")\"\n",
    "        )\n",
    "        insert_sql = (\n",
    "            f\"INSERT INTO raw ({', '.join(qi(c) for c in columns)}) VALUES \"\n",
    "            f\"({', '.join(['?'] * len(columns))})\"\n",
    "        )\n",
    "\n",
    "        stats = {\n",
    "            c: {\"total\": 0, \"non_missing\": 0, \"missing\": 0, \"numeric_hits\": 0, \"datetime_hits\": 0}\n",
    "            for c in columns\n",
    "        }\n",
    "\n",
    "        def normalize_row(row: Dict[str, str]) -> Tuple[str, ...]:\n",
    "            vals: List[str] = []\n",
    "            for col in columns:\n",
    "                raw_val = row.get(col, \"\")\n",
    "                val = \"\" if raw_val is None else str(raw_val).strip()\n",
    "                vals.append(val)\n",
    "            return tuple(vals)\n",
    "\n",
    "        def update_stats(values: Sequence[str]) -> None:\n",
    "            for i, val in enumerate(values):\n",
    "                col = columns[i]\n",
    "                st = stats[col]\n",
    "                st[\"total\"] += 1\n",
    "                if val == \"\":\n",
    "                    st[\"missing\"] += 1\n",
    "                    continue\n",
    "                st[\"non_missing\"] += 1\n",
    "                if parse_float(val) is not None:\n",
    "                    st[\"numeric_hits\"] += 1\n",
    "                if parse_datetime(val) is not None:\n",
    "                    st[\"datetime_hits\"] += 1\n",
    "\n",
    "        first_vals = normalize_row(first)\n",
    "        update_stats(first_vals)\n",
    "        batch: List[Tuple[str, ...]] = [first_vals]\n",
    "        batch_size = 5000\n",
    "\n",
    "        for row in rows:\n",
    "            vals = normalize_row(row)\n",
    "            update_stats(vals)\n",
    "            batch.append(vals)\n",
    "            if len(batch) >= batch_size:\n",
    "                conn.executemany(insert_sql, batch)\n",
    "                batch.clear()\n",
    "        if batch:\n",
    "            conn.executemany(insert_sql, batch)\n",
    "        conn.commit()\n",
    "\n",
    "        row_count = conn.execute(\"SELECT COUNT(*) FROM raw\").fetchone()[0]\n",
    "        inferred = infer_types(stats)\n",
    "\n",
    "        head_rows = []\n",
    "        cur = conn.execute(\"SELECT * FROM raw LIMIT 5\")\n",
    "        head_cols = [d[0] for d in cur.description]\n",
    "        for row in cur.fetchall():\n",
    "            head_rows.append({head_cols[i]: row[i] for i in range(len(head_cols))})\n",
    "\n",
    "        missing_before = {c: int(stats[c][\"missing\"]) for c in columns}\n",
    "\n",
    "        if args.missing_strategy == \"drop\":\n",
    "            non_null_checks = \" AND \".join(f\"{qi(c)} IS NOT NULL AND TRIM({qi(c)}) <> ''\" for c in columns)\n",
    "            conn.execute(f\"CREATE TABLE cleaned AS SELECT * FROM raw WHERE {non_null_checks}\")\n",
    "        else:\n",
    "            select_exprs = []\n",
    "            for c in columns:\n",
    "                dtype = inferred[c]\n",
    "                if dtype == \"numeric\":\n",
    "                    select_exprs.append(f\"safe_real({qi(c)}) AS {qi(c)}\")\n",
    "                elif dtype == \"datetime\":\n",
    "                    select_exprs.append(f\"parse_datetime({qi(c)}) AS {qi(c)}\")\n",
    "                else:\n",
    "                    select_exprs.append(f\"NULLIF(TRIM({qi(c)}), '') AS {qi(c)}\")\n",
    "            conn.execute(\"CREATE TABLE cleaned AS SELECT \" + \", \".join(select_exprs) + \" FROM raw\")\n",
    "\n",
    "            for c in columns:\n",
    "                qcol = qi(c)\n",
    "                dtype = inferred[c]\n",
    "                if dtype == \"numeric\":\n",
    "                    mean_val = conn.execute(f\"SELECT AVG({qcol}) FROM cleaned WHERE {qcol} IS NOT NULL\").fetchone()[0]\n",
    "                    fill_val = float(mean_val) if mean_val is not None else 0.0\n",
    "                    conn.execute(f\"UPDATE cleaned SET {qcol} = ? WHERE {qcol} IS NULL\", (fill_val,))\n",
    "                elif dtype == \"datetime\":\n",
    "                    mode_val = conn.execute(\n",
    "                        f\"SELECT {qcol} FROM cleaned WHERE {qcol} IS NOT NULL \"\n",
    "                        f\"GROUP BY {qcol} ORDER BY COUNT(*) DESC LIMIT 1\"\n",
    "                    ).fetchone()\n",
    "                    fill_val = mode_val[0] if mode_val else \"1970-01-01 00:00:00\"\n",
    "                    conn.execute(f\"UPDATE cleaned SET {qcol} = ? WHERE {qcol} IS NULL\", (fill_val,))\n",
    "                else:\n",
    "                    mode_val = conn.execute(\n",
    "                        f\"SELECT {qcol} FROM cleaned WHERE {qcol} IS NOT NULL \"\n",
    "                        f\"GROUP BY {qcol} ORDER BY COUNT(*) DESC LIMIT 1\"\n",
    "                    ).fetchone()\n",
    "                    fill_val = mode_val[0] if mode_val else \"Unknown\"\n",
    "                    conn.execute(f\"UPDATE cleaned SET {qcol} = ? WHERE {qcol} IS NULL\", (fill_val,))\n",
    "\n",
    "        cleaned_before_dedup = conn.execute(\"SELECT COUNT(*) FROM cleaned\").fetchone()[0]\n",
    "        conn.execute(\"CREATE TABLE cleaned_dedup AS SELECT DISTINCT * FROM cleaned\")\n",
    "        conn.execute(\"DROP TABLE cleaned\")\n",
    "        conn.execute(\"ALTER TABLE cleaned_dedup RENAME TO cleaned\")\n",
    "        cleaned_after_dedup = conn.execute(\"SELECT COUNT(*) FROM cleaned\").fetchone()[0]\n",
    "        duplicates_removed = cleaned_before_dedup - cleaned_after_dedup\n",
    "\n",
    "        missing_after = {}\n",
    "        for c in columns:\n",
    "            qcol = qi(c)\n",
    "            missing_after[c] = conn.execute(\n",
    "                f\"SELECT COUNT(*) FROM cleaned WHERE {qcol} IS NULL OR TRIM(CAST({qcol} AS TEXT)) = ''\"\n",
    "            ).fetchone()[0]\n",
    "\n",
    "        numeric_columns = [c for c in columns if inferred[c] == \"numeric\"]\n",
    "        categorical_columns = [c for c in columns if inferred[c] == \"categorical\"]\n",
    "        datetime_columns = [c for c in columns if inferred[c] == \"datetime\"]\n",
    "\n",
    "        describe = {}\n",
    "        for c in numeric_columns:\n",
    "            stats_row = describe_numeric_column(conn, \"cleaned\", c)\n",
    "            if stats_row:\n",
    "                describe[c] = stats_row\n",
    "\n",
    "        conn.execute(\"CREATE TABLE transformed AS SELECT * FROM cleaned\")\n",
    "\n",
    "        scaling = {}\n",
    "        for c in numeric_columns:\n",
    "            qcol = qi(c)\n",
    "            mean_, avg_sq = conn.execute(\n",
    "                f\"SELECT AVG({qcol}), AVG({qcol} * {qcol}) FROM transformed\"\n",
    "            ).fetchone()\n",
    "            mean_val = float(mean_ or 0.0)\n",
    "            variance = max(float(avg_sq or 0.0) - mean_val * mean_val, 0.0)\n",
    "            std_val = math.sqrt(variance)\n",
    "            zcol = f\"{c}_z\"\n",
    "            conn.execute(f\"ALTER TABLE transformed ADD COLUMN {qi(zcol)} REAL\")\n",
    "            if std_val > 0:\n",
    "                conn.execute(\n",
    "                    f\"UPDATE transformed SET {qi(zcol)} = ({qcol} - ?) / ?\",\n",
    "                    (mean_val, std_val),\n",
    "                )\n",
    "            else:\n",
    "                conn.execute(f\"UPDATE transformed SET {qi(zcol)} = 0.0\")\n",
    "            scaling[c] = {\"mean\": mean_val, \"std\": std_val}\n",
    "\n",
    "        encoding_summary = {\"one_hot\": {}, \"label_encoded\": {}}\n",
    "        added_columns = set(columns + [f\"{c}_z\" for c in numeric_columns])\n",
    "\n",
    "        for idx, c in enumerate(categorical_columns):\n",
    "            qcol = qi(c)\n",
    "            unique_count = conn.execute(f\"SELECT COUNT(DISTINCT {qcol}) FROM transformed\").fetchone()[0]\n",
    "            can_one_hot = (not is_id_like(c)) and 1 < unique_count <= int(args.onehot_max_levels)\n",
    "\n",
    "            if can_one_hot:\n",
    "                categories = [\n",
    "                    row[0]\n",
    "                    for row in conn.execute(\n",
    "                        f\"SELECT DISTINCT {qcol} FROM transformed ORDER BY {qcol}\"\n",
    "                    ).fetchall()\n",
    "                ]\n",
    "                out_cols = []\n",
    "                for cat in categories:\n",
    "                    base = f\"{c}__{slugify(cat)}\"\n",
    "                    new_col = base\n",
    "                    suffix = 2\n",
    "                    while new_col in added_columns:\n",
    "                        new_col = f\"{base}_{suffix}\"\n",
    "                        suffix += 1\n",
    "                    added_columns.add(new_col)\n",
    "                    out_cols.append(new_col)\n",
    "\n",
    "                    conn.execute(f\"ALTER TABLE transformed ADD COLUMN {qi(new_col)} INTEGER\")\n",
    "                    conn.execute(\n",
    "                        f\"UPDATE transformed SET {qi(new_col)} = CASE WHEN {qcol} = ? THEN 1 ELSE 0 END\",\n",
    "                        (cat,),\n",
    "                    )\n",
    "                encoding_summary[\"one_hot\"][c] = out_cols\n",
    "            else:\n",
    "                enc_col = f\"{c}__encoded\"\n",
    "                if enc_col in added_columns:\n",
    "                    enc_col = f\"{enc_col}_{idx+1}\"\n",
    "                added_columns.add(enc_col)\n",
    "                conn.execute(f\"ALTER TABLE transformed ADD COLUMN {qi(enc_col)} INTEGER\")\n",
    "\n",
    "                map_table = f\"map_{idx}_{c}\"\n",
    "                map_table = re.sub(r\"[^0-9a-zA-Z_]+\", \"_\", map_table)\n",
    "                conn.execute(f\"CREATE TEMP TABLE {qi(map_table)} (value TEXT PRIMARY KEY, code INTEGER)\")\n",
    "                labels = conn.execute(\n",
    "                    f\"SELECT DISTINCT {qcol} FROM transformed ORDER BY {qcol}\"\n",
    "                ).fetchall()\n",
    "                conn.executemany(\n",
    "                    f\"INSERT INTO {qi(map_table)} (value, code) VALUES (?, ?)\",\n",
    "                    [(r[0], i) for i, r in enumerate(labels)],\n",
    "                )\n",
    "                conn.execute(\n",
    "                    f\"UPDATE transformed SET {qi(enc_col)} = (SELECT code FROM {qi(map_table)} m WHERE m.value = {qcol})\"\n",
    "                )\n",
    "                conn.execute(f\"DROP TABLE {qi(map_table)}\")\n",
    "                encoding_summary[\"label_encoded\"][c] = enc_col\n",
    "\n",
    "        engineered = []\n",
    "        if \"quantity\" in columns and \"unitprice\" in columns:\n",
    "            conn.execute(\"ALTER TABLE transformed ADD COLUMN total_amount REAL\")\n",
    "            conn.execute(\"UPDATE transformed SET total_amount = quantity * unitprice\")\n",
    "            engineered.append(\"total_amount\")\n",
    "\n",
    "        if \"quantity\" in columns:\n",
    "            conn.execute(\"ALTER TABLE transformed ADD COLUMN is_return INTEGER\")\n",
    "            conn.execute(\"UPDATE transformed SET is_return = CASE WHEN quantity < 0 THEN 1 ELSE 0 END\")\n",
    "            engineered.append(\"is_return\")\n",
    "\n",
    "        if \"invoicedate\" in columns:\n",
    "            conn.execute(\"ALTER TABLE transformed ADD COLUMN invoice_year INTEGER\")\n",
    "            conn.execute(\"ALTER TABLE transformed ADD COLUMN invoice_month INTEGER\")\n",
    "            conn.execute(\"ALTER TABLE transformed ADD COLUMN invoice_day INTEGER\")\n",
    "            conn.execute(\"ALTER TABLE transformed ADD COLUMN invoice_hour INTEGER\")\n",
    "            conn.execute(\"ALTER TABLE transformed ADD COLUMN invoice_weekday INTEGER\")\n",
    "            conn.execute(\n",
    "                \"UPDATE transformed SET \"\n",
    "                \"invoice_year = CAST(strftime('%Y', invoicedate) AS INTEGER), \"\n",
    "                \"invoice_month = CAST(strftime('%m', invoicedate) AS INTEGER), \"\n",
    "                \"invoice_day = CAST(strftime('%d', invoicedate) AS INTEGER), \"\n",
    "                \"invoice_hour = CAST(strftime('%H', invoicedate) AS INTEGER), \"\n",
    "                \"invoice_weekday = CAST(strftime('%w', invoicedate) AS INTEGER)\"\n",
    "            )\n",
    "            engineered.extend(\n",
    "                [\"invoice_year\", \"invoice_month\", \"invoice_day\", \"invoice_hour\", \"invoice_weekday\"]\n",
    "            )\n",
    "\n",
    "        target_candidates = [\"total_amount\", \"sales_amount\", \"revenue\", \"unitprice\"]\n",
    "        target = next((t for t in target_candidates if conn.execute(\n",
    "            \"SELECT COUNT(*) FROM pragma_table_info('transformed') WHERE name = ?\",\n",
    "            (t,),\n",
    "        ).fetchone()[0]), None)\n",
    "\n",
    "        table_info = conn.execute(\"PRAGMA table_info(transformed)\").fetchall()\n",
    "        numeric_like_cols = [\n",
    "            row[1]\n",
    "            for row in table_info\n",
    "            if row[2].upper().startswith(\"INT\") or row[2].upper().startswith(\"REAL\")\n",
    "        ]\n",
    "\n",
    "        feature_scores = []\n",
    "        if target and target in numeric_like_cols:\n",
    "            qt = qi(target)\n",
    "            for c in numeric_like_cols:\n",
    "                if c == target:\n",
    "                    continue\n",
    "                qc = qi(c)\n",
    "                avgx, avgy, avgxy, avgxx, avgyy = conn.execute(\n",
    "                    f\"SELECT AVG({qc}), AVG({qt}), AVG({qc}*{qt}), AVG({qc}*{qc}), AVG({qt}*{qt}) \"\n",
    "                    \"FROM transformed\"\n",
    "                ).fetchone()\n",
    "                if avgx is None or avgy is None:\n",
    "                    continue\n",
    "                varx = max((avgxx or 0.0) - (avgx or 0.0) ** 2, 0.0)\n",
    "                vary = max((avgyy or 0.0) - (avgy or 0.0) ** 2, 0.0)\n",
    "                if varx <= 1e-12 or vary <= 1e-12:\n",
    "                    continue\n",
    "                cov = (avgxy or 0.0) - (avgx or 0.0) * (avgy or 0.0)\n",
    "                corr = cov / math.sqrt(varx * vary)\n",
    "                feature_scores.append((c, abs(corr), varx))\n",
    "            feature_scores.sort(key=lambda x: (x[1], x[2]), reverse=True)\n",
    "        else:\n",
    "            for c in numeric_like_cols:\n",
    "                qc = qi(c)\n",
    "                avg_, avg_sq = conn.execute(\n",
    "                    f\"SELECT AVG({qc}), AVG({qc}*{qc}) FROM transformed\"\n",
    "                ).fetchone()\n",
    "                if avg_ is None:\n",
    "                    continue\n",
    "                var_ = max((avg_sq or 0.0) - (avg_ or 0.0) ** 2, 0.0)\n",
    "                if var_ <= 1e-12:\n",
    "                    continue\n",
    "                feature_scores.append((c, 0.0, var_))\n",
    "            feature_scores.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "        top_k = max(int(args.top_features), 1)\n",
    "        selected = [x[0] for x in feature_scores[:top_k]]\n",
    "        if target and target not in selected:\n",
    "            selected.append(target)\n",
    "        if not selected:\n",
    "            selected = numeric_like_cols[:top_k]\n",
    "\n",
    "        conn.execute(\"DROP TABLE IF EXISTS selected_features\")\n",
    "        conn.execute(\n",
    "            \"CREATE TABLE selected_features AS SELECT \"\n",
    "            + \", \".join(qi(c) for c in selected)\n",
    "            + \" FROM transformed\"\n",
    "        )\n",
    "        conn.commit()\n",
    "\n",
    "        cleaned_csv = output_dir / \"cleaned_data.csv\"\n",
    "        transformed_preview_csv = output_dir / \"transformed_preview.csv\"\n",
    "        selected_csv = output_dir / \"selected_features.csv\"\n",
    "        report_json = output_dir / \"preparation_report.json\"\n",
    "\n",
    "        export_table_csv(conn, \"cleaned\", cleaned_csv)\n",
    "        export_table_csv(conn, \"transformed\", transformed_preview_csv, limit=2000)\n",
    "        export_table_csv(conn, \"selected_features\", selected_csv)\n",
    "\n",
    "        report = {\n",
    "            \"source\": str(source_path),\n",
    "            \"output_directory\": str(output_dir),\n",
    "            \"row_counts\": {\n",
    "                \"raw\": int(row_count),\n",
    "                \"cleaned_before_dedup\": int(cleaned_before_dedup),\n",
    "                \"cleaned_after_dedup\": int(cleaned_after_dedup),\n",
    "                \"duplicates_removed\": int(duplicates_removed),\n",
    "            },\n",
    "            \"head\": head_rows,\n",
    "            \"info\": {\n",
    "                \"columns\": columns,\n",
    "                \"inferred_types\": inferred,\n",
    "            },\n",
    "            \"missing_values\": {\n",
    "                \"before_cleaning\": missing_before,\n",
    "                \"after_cleaning\": missing_after,\n",
    "            },\n",
    "            \"describe\": describe,\n",
    "            \"transform\": {\n",
    "                \"standardized_numeric_columns\": list(numeric_columns),\n",
    "                \"scaling_parameters\": scaling,\n",
    "                \"categorical_encoding\": encoding_summary,\n",
    "            },\n",
    "            \"feature_engineering\": {\n",
    "                \"created_features\": engineered,\n",
    "            },\n",
    "            \"feature_selection\": {\n",
    "                \"target\": target,\n",
    "                \"selected_features\": selected,\n",
    "                \"top_scores\": [\n",
    "                    {\"feature\": f, \"abs_corr\": float(c), \"variance\": float(v)}\n",
    "                    for f, c, v in feature_scores[:top_k]\n",
    "                ],\n",
    "            },\n",
    "            \"artifacts\": {\n",
    "                \"cleaned_data_csv\": str(cleaned_csv),\n",
    "                \"transformed_preview_csv\": str(transformed_preview_csv),\n",
    "                \"selected_features_csv\": str(selected_csv),\n",
    "                \"work_db\": str(db_path),\n",
    "            },\n",
    "        }\n",
    "\n",
    "        report_json.write_text(json.dumps(report, indent=2), encoding=\"utf-8\")\n",
    "        report[\"artifacts\"][\"report_json\"] = str(report_json)\n",
    "        return report\n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "\n",
    "def build_parser() -> argparse.ArgumentParser:\n",
    "    parser = argparse.ArgumentParser(description=\"Generic data preparation pipeline.\")\n",
    "    parser.add_argument(\"--input\", required=True, help=\"Input path (.csv, .xlsx, .db/.sqlite)\")\n",
    "    parser.add_argument(\n",
    "        \"--source-type\",\n",
    "        choices=[\"auto\", \"csv\", \"excel\", \"sql\"],\n",
    "        default=\"auto\",\n",
    "        help=\"Source type. Default auto-infers from file extension.\",\n",
    "    )\n",
    "    parser.add_argument(\"--sheet\", default=None, help=\"Excel sheet name (optional).\")\n",
    "    parser.add_argument(\"--sql-query\", default=None, help=\"SQL query for SQL sources.\")\n",
    "    parser.add_argument(\"--sql-table\", default=None, help=\"SQL table for SQL sources.\")\n",
    "    parser.add_argument(\n",
    "        \"--missing-strategy\",\n",
    "        choices=[\"fill\", \"drop\"],\n",
    "        default=\"fill\",\n",
    "        help=\"Missing-value strategy.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--onehot-max-levels\",\n",
    "        type=int,\n",
    "        default=40,\n",
    "        help=\"Max distinct categories to one-hot encode.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--top-features\",\n",
    "        type=int,\n",
    "        default=12,\n",
    "        help=\"Number of features to keep in selected_features output.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--output-dir\",\n",
    "        default=\"./data_prep_output\",\n",
    "        help=\"Directory for output artifacts.\",\n",
    "    )\n",
    "    return parser\n",
    "\n",
    "\n",
    "def main() -> None:\n",
    "    parser = build_parser()\n",
    "    args = parser.parse_args()\n",
    "    report = run_pipeline(args)\n",
    "    print(\"Data preparation complete.\")\n",
    "    print(f\"Input source: {report['source']}\")\n",
    "    print(f\"Rows loaded: {report['row_counts']['raw']}\")\n",
    "    print(f\"Rows after cleaning/dedup: {report['row_counts']['cleaned_after_dedup']}\")\n",
    "    print(f\"Duplicates removed: {report['row_counts']['duplicates_removed']}\")\n",
    "    print(\"Selected features:\", \", \".join(report[\"feature_selection\"][\"selected_features\"]))\n",
    "    print(\"Artifacts:\")\n",
    "    for k, v in report[\"artifacts\"].items():\n",
    "        print(f\"  - {k}: {v}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}